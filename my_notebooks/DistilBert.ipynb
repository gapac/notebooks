{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "- Overview: DistilBERT is a smaller, faster, and lighter version of BERT, trained using knowledge distillation to retain much of the performance of the full BERT model.\n",
    "- Size: About 66 million parameters, making it around 40% smaller and 60% faster than BERT.\n",
    "- Usage: It can perform named entity recognition (NER) to identify objects and relevant attributes (like colors and positions) in sentences. You can fine-tune DistilBERT on a small dataset of descriptions to extract objects and attributes with higher precision.\n",
    "- Implementation: Using Hugging Faceâ€™s Transformers library, DistilBERT can be fine-tuned for entity recognition on your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Load DistilBERT for fill-mask (masked language modeling)\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\", device=device)\n",
    "\n",
    "# Sample description text\n",
    "description = \"In this image, I can see a dog which is black, brown, and white in color laying on the ground. \" \\\n",
    "              \"I can also see two balls which are green and purple in color on the grass. \" \\\n",
    "              \"In the background, there are a few plants which are purple and blue, the ground, a few trees, and the sky.\"\n",
    "\n",
    "# Define a list to store extracted objects and their attributes\n",
    "objects = []\n",
    "\n",
    "# Define function to identify objects and attributes\n",
    "def extract_objects(description):\n",
    "    # Define prompts to extract objects and their colors\n",
    "    sentences = [\n",
    "        f\"In the image, there is a [MASK].\",\n",
    "        f\"The [MASK] is black, brown, and white.\",\n",
    "        f\"I see [MASK] balls which are green and purple.\",\n",
    "        f\"There are [MASK] plants which are purple and blue.\",\n",
    "    ]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        results = fill_mask(sentence)\n",
    "        # Filter top prediction\n",
    "        best_match = results[0]['token_str']\n",
    "        objects.append(best_match)\n",
    "    \n",
    "    return objects\n",
    "\n",
    "# Run the function\n",
    "extracted_objects = extract_objects(description)\n",
    "\n",
    "# Print extracted objects and their descriptions\n",
    "print(\"Extracted Objects and Attributes:\")\n",
    "for obj in extracted_objects:\n",
    "    print(f\"Object: {obj}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load MiniLM for question answering\n",
    "model_name = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Define a list to store extracted objects and their attributes\n",
    "objects = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilbert and Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/home/g22/GitHub/notebooks/env/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/g22/GitHub/notebooks/env/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 57, in main\n",
      "    service.run()\n",
      "  File \"/home/g22/GitHub/notebooks/env/lib/python3.10/site-packages/huggingface_hub/commands/user.py\", line 153, in run\n",
      "    login(\n",
      "  File \"/home/g22/GitHub/notebooks/env/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 123, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/home/g22/GitHub/notebooks/env/lib/python3.10/site-packages/huggingface_hub/_login.py\", line 275, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \"/usr/lib/python3.10/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "# Load spaCy and DistilBERT models\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")  # Lightweight spaCy model for parsing\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"elastic/distilbert-base-uncased-finetuned-conll03-english\")\n",
    "distilbert_model = DistilBertForTokenClassification.from_pretrained(\"elastic/distilbert-base-uncased-finetuned-conll03-english\")\n",
    "\n",
    "# Use Hugging Face pipeline for NER with DistilBERT\n",
    "ner_pipeline = pipeline(\"ner\", model=distilbert_model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "# Sample input text\n",
    "text = \"\"\"In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "I can also see two balls which are green and purple in color on the grass.\n",
    "In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\"\"\"\n",
    "\n",
    "# Step 1: Extract Entities Using DistilBERT NER\n",
    "# Step 1: Extract Entities Using DistilBERT NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Print extracted entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code with SpaCy for NER and extracting object attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'label_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m entity_info \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Check the entity type and assign relevant attributes\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     23\u001b[0m     entity_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ent\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     24\u001b[0m     entity_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrganization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'label_'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with SpaCy NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize a list to hold the extracted objects with their attributes\n",
    "extracted_objects = []\n",
    "\n",
    "# Loop through the entities identified by SpaCy\n",
    "for ent in doc.ents:\n",
    "    entity_info = {}\n",
    "    \n",
    "    # Check the entity type and assign relevant attributes\n",
    "    if ent.label_ == \"ORG\":\n",
    "        entity_info[\"entity\"] = ent.text\n",
    "        entity_info[\"type\"] = \"Organization\"\n",
    "        # Check if there is a location mentioned for the organization\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"pobj\" and token.head.text in [\"in\", \"at\"]:\n",
    "                entity_info[\"location\"] = token.text\n",
    "\n",
    "    elif ent.label_ == \"PRODUCT\":\n",
    "        entity_info[\"entity\"] = ent.text\n",
    "        entity_info[\"type\"] = \"Product\"\n",
    "        # Check if a price is mentioned after the product\n",
    "        for token in doc:\n",
    "            if token.text.startswith(\"$\"):\n",
    "                entity_info[\"price\"] = token.text\n",
    "    \n",
    "    elif ent.label_ == \"GPE\":  # Geopolitical Entity (Country, City, etc.)\n",
    "        entity_info[\"entity\"] = ent.text\n",
    "        entity_info[\"type\"] = \"Location\"\n",
    "\n",
    "    elif ent.label_ == \"PERSON\":\n",
    "        entity_info[\"entity\"] = ent.text\n",
    "        entity_info[\"type\"] = \"Person\"  \n",
    "\n",
    "    elif ent.label_ == \"OBJECT\":\n",
    "        entity_info[\"entity\"] = ent.text\n",
    "        entity_info[\"type\"] = \"Object\"\n",
    "        # Check for color and quantity descriptors\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"amod\" and token.head.text == ent.text:\n",
    "                entity_info[\"color\"] = token.text\n",
    "            elif token.dep_ == \"nummod\" and token.head.text == ent.text:\n",
    "                entity_info[\"quantity\"] = token.text\n",
    "    \n",
    "    # Add the object with its attributes to the list\n",
    "    if entity_info:\n",
    "        extracted_objects.append(entity_info)\n",
    "\n",
    "# Display the extracted objects and their attributes\n",
    "for obj in extracted_objects:\n",
    "    print(obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
      "\n",
      "Extracted objects: ['dog']\n",
      "\n",
      "Sentence: I can also see two balls which are green and purple in color on the grass.\n",
      "\n",
      "Extracted objects: ['ball']\n",
      "\n",
      "Sentence: In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\n",
      "\n",
      "Extracted objects: ['plant']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract objects from a paragraph and make them singular\n",
    "def extract_objects_from_paragraph(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    objects = []\n",
    "    for sent in doc.sents:  # Iterate through each sentence in the paragraph\n",
    "        sentence_objects = []\n",
    "        for token in sent:\n",
    "            # Check if the token is an object (direct, indirect, or prepositional)\n",
    "            if token.dep_ in (\"dobj\", \"iobj\"):#, \"pobj\"):\n",
    "                # Append the lemmatized form of the object (singular form)\n",
    "                sentence_objects.append(token.lemma_)\n",
    "        objects.append((sent.text, sentence_objects))\n",
    "    return objects\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = \"\"\"\n",
    "In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "I can also see two balls which are green and purple in color on the grass.\n",
    "In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\n",
    "\"\"\"\n",
    "\n",
    "# Extract objects\n",
    "objects = extract_objects_from_paragraph(paragraph)\n",
    "for sentence, obj_list in objects:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Extracted objects: {obj_list}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined objects: book friend library professor project assignment classroom exam\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract objects from a paragraph, make them singular, and combine them into one string\n",
    "def extract_and_combine_objects(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    all_objects = []\n",
    "    for sent in doc.sents:  # Iterate through each sentence in the paragraph\n",
    "        for token in sent:\n",
    "            # Check if the token is an object (direct, indirect, or prepositional)\n",
    "            if token.dep_ in (\"dobj\", \"iobj\", \"pobj\"):\n",
    "                # Append the lemmatized form of the object (singular form)\n",
    "                all_objects.append(token.lemma_)\n",
    "    \n",
    "    # Join all objects into a single string\n",
    "    combined_objects = ' '.join(all_objects)\n",
    "    return combined_objects\n",
    "\n",
    "# Example paragraph\n",
    "paragraph = \"\"\"\n",
    "She gave the books to her friends in the libraries. Later, she met with her professors to discuss the projects. \n",
    "The students were completing their assignments in the classrooms while the teachers graded exams.\n",
    "\"\"\"\n",
    "\n",
    "# Extract and combine objects into a single string\n",
    "combined_objects = extract_and_combine_objects(paragraph)\n",
    "print(f\"Combined objects: {combined_objects}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I can see a brown box on the floor.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['I', 'a dog', 'which', 'the ground']\n",
      "Verbs: ['see', 'lay']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"I can see a dog which is black, brown and white in color laying on the ground.\")\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: dog\n",
      "Attributes: {'color': None, 'position': None, 'quantity': 1}\n",
      "\n",
      "Object: color\n",
      "Attributes: {'color': None, 'position': None, 'quantity': 1}\n",
      "\n",
      "Object: ground\n",
      "Attributes: {'color': None, 'position': None, 'quantity': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"I can see a dog which is black, brown and white in color laying on the ground.\"\"\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp_spacy(text)\n",
    "\n",
    "# Define a function to extract the objects and attributes\n",
    "def extract_objects_and_attributes(doc):\n",
    "    extracted_info = {}\n",
    "    \n",
    "    # Iterate over the tokens in the document\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":  # Object candidates are nouns and proper nouns\n",
    "            object_name = token.lemma_.lower()\n",
    "\n",
    "            if object_name not in extracted_info:\n",
    "                extracted_info[object_name] = {\"color\": [], \"position\": [], \"quantity\": None}\n",
    "            \n",
    "            # Extract adjectives (color) and quantities\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\" and child.pos_ == \"ADJ\":  # Color adjectives\n",
    "                    extracted_info[object_name][\"color\"].append(child.text)\n",
    "                elif child.dep_ == \"nummod\":  # Quantities\n",
    "                    extracted_info[object_name][\"quantity\"] = child.text\n",
    "                elif child.dep_ == \"prep\":  # Prepositions indicating position\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.dep_ == \"pobj\" and grandchild.pos_ == \"NOUN\":\n",
    "                            extracted_info[object_name][\"position\"].append(grandchild.text)\n",
    "\n",
    "    # Clean up results, removing unnecessary empty values\n",
    "    for obj in extracted_info:\n",
    "        if not extracted_info[obj][\"color\"]:\n",
    "            extracted_info[obj][\"color\"] = None\n",
    "        if not extracted_info[obj][\"position\"]:\n",
    "            extracted_info[obj][\"position\"] = None\n",
    "        if not extracted_info[obj][\"quantity\"]:\n",
    "            extracted_info[obj][\"quantity\"] = 1  # Default to 1 if not specified\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "# Run the extraction function\n",
    "extracted_info = extract_objects_and_attributes(doc)\n",
    "for obj, attributes in extracted_info.items():\n",
    "    print(f\"Object: {obj}\")\n",
    "    print(f\"Attributes: {attributes}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'color': None, 'position': None, 'quantity': 1}, 'dog': {'color': None, 'position': None, 'quantity': 1}, 'color': {'color': None, 'position': None, 'quantity': 1}, 'ground': {'color': None, 'position': None, 'quantity': 1}, 'ball': {'color': None, 'position': None, 'quantity': 'two'}, 'grass': {'color': None, 'position': None, 'quantity': 1}, 'background': {'color': None, 'position': None, 'quantity': 1}, 'plant': {'color': None, 'position': None, 'quantity': 'few'}, 'tree': {'color': ['few'], 'position': None, 'quantity': 1}, 'sky': {'color': None, 'position': None, 'quantity': 1}}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "I can also see two balls which are green and purple in color on the grass.\n",
    "In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\"\"\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp_spacy(text)\n",
    "\n",
    "# Define a function to extract the objects and attributes\n",
    "def extract_objects_and_attributes(doc):\n",
    "    extracted_info = {}\n",
    "\n",
    "    # Iterate over the tokens in the document\n",
    "    for token in doc:\n",
    "        # Object candidates are nouns and proper nouns\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":  \n",
    "            object_name = token.lemma_.lower()\n",
    "\n",
    "            if object_name not in extracted_info:\n",
    "                extracted_info[object_name] = {\"color\": [], \"position\": [], \"quantity\": None}\n",
    "\n",
    "            # Extract adjectives (color) and quantities\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\" and child.pos_ == \"ADJ\":  # Color adjectives\n",
    "                    extracted_info[object_name][\"color\"].append(child.text)\n",
    "                elif child.dep_ == \"nummod\":  # Quantities\n",
    "                    extracted_info[object_name][\"quantity\"] = child.text\n",
    "                elif child.dep_ == \"prep\" and child.pos_ == \"NOUN\":  # Positioning (e.g., \"on the grass\")\n",
    "                    extracted_info[object_name][\"position\"].append(child.text)\n",
    "\n",
    "    # Extracting more complex color phrases (like \"black, brown and white in color\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"COLOR\":  # Handle color-based entities\n",
    "            object_name = ent.text.lower()\n",
    "            for key in extracted_info:\n",
    "                if key in object_name:\n",
    "                    extracted_info[key][\"color\"] = extracted_info[key].get(\"color\", []) + [ent.text]\n",
    "\n",
    "    # Clean up results, removing unnecessary empty values\n",
    "    for obj in extracted_info:\n",
    "        if not extracted_info[obj][\"color\"]:\n",
    "            extracted_info[obj][\"color\"] = None\n",
    "        if not extracted_info[obj][\"position\"]:\n",
    "            extracted_info[obj][\"position\"] = None\n",
    "        if not extracted_info[obj][\"quantity\"]:\n",
    "            extracted_info[obj][\"quantity\"] = 1  # Default to 1 if not specified\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "# Run the extraction function\n",
    "extracted_info = extract_objects_and_attributes(doc)\n",
    "print(extracted_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/g22/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/g22/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/g22/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/g22/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/g22/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "# Ensure you have the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "text = \"\"\"In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "I can also see two balls which are green and purple in color on the grass.\n",
    "In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\"\"\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Function to extract noun phrases\n",
    "def extract_objects(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    chunked = ne_chunk(pos_tags)\n",
    "    iob_tagged = tree2conlltags(chunked)\n",
    "    \n",
    "    objects = []\n",
    "    for word, pos, chunk in iob_tagged:\n",
    "        if chunk == 'B-NP' or chunk == 'I-NP':\n",
    "            objects.append(word)\n",
    "    return objects\n",
    "\n",
    "# Extract objects from each sentence\n",
    "all_objects = []\n",
    "for sentence in sentences:\n",
    "    objects = extract_objects(sentence)\n",
    "    all_objects.extend(objects)\n",
    "\n",
    "# Print the extracted objects\n",
    "print(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this image', 'I', 'a dog', 'which', 'the ground', 'I', 'two balls', 'which', 'color', 'the grass', 'the background', 'I', 'a few plants', 'which', 'color', 'the ground', 'a few trees', 'the sky']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Install spaCy and download the English model\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"In this image I can see a dog which is black, brown and white in color laying on the ground.\n",
    "I can also see two balls which are green and purple in color on the grass.\n",
    "In the background I can see a few plants which are purple and blue in color, the ground, a few trees, and the sky.\"\"\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract objects based on dependency parsing\n",
    "objects = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# Print the extracted objects\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
